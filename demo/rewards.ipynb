{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward mechanism demo\n",
    "\n",
    "## Imports + Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chunking.protocol import chunkSynapse\n",
    "import bittensor as bt\n",
    "import requests\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install python-dotenv -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI API key necessary for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if not os.environ.get('OPENAI_API_KEY'):\n",
    "    raise Exception(\"Make sure to set OPENAI_API_KEY in your .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.27187   , 0.31795225, 0.1597467 , 0.14297703, 0.09549096,\n",
       "       0.00793469, 0.00399786], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netuid = 1 # TODO: set netuid\n",
    "network = 'ws://localhost:9946' # TODO: set network\n",
    "\n",
    "metagraph = bt.metagraph(netuid, network)\n",
    "\n",
    "metagraph.I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wallet setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallet_name = \"owner\" # TODO: set wallet name\n",
    "hotkey_name = \"validator-1\" # TODO: set hotkey name\n",
    "\n",
    "validator_wallet = bt.wallet(name=wallet_name, hotkey=hotkey_name)\n",
    "validator_dendrite = bt.dendrite(wallet=validator_wallet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the top miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top miner uid: 1\n",
      "Document: ? (also written Tanda Tanya, meaning Question Mark) is a 2011 Indonesian drama film directed by Hanu ...\n",
      "Received 5 chunks from top miner, process time: 0.04235506057739258\n",
      "Chunk 3: The director of Mahaka Pictures, Erick Thohir, sta ...\n"
     ]
    }
   ],
   "source": [
    "page = 33653136 # hard article id\n",
    "\n",
    "def generate_synthetic_synapse() -> chunkSynapse:    \n",
    "    document = requests.get('https://en.wikipedia.org/w/api.php', params={\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'pageids': page,\n",
    "        'prop': 'extracts',\n",
    "        'explaintext': True,\n",
    "        'exsectionformat': 'plain',\n",
    "        }).json()['query']['pages'][str(page)]['extract']\n",
    "    document = document.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    document = ' '.join(document.split())\n",
    "    synapse = chunkSynapse(document=document, time_soft_max=5.0, chunk_size=4096)\n",
    "    return synapse\n",
    "\n",
    "\n",
    "top_miner_uid = metagraph.I.argmax().item()\n",
    "\n",
    "print(f\"Top miner uid: {top_miner_uid}\")\n",
    "\n",
    "top_miner_axon = metagraph.axons[top_miner_uid]\n",
    "\n",
    "# Generate the 'synthetic' query: a featured article from wikipedia.\n",
    "synapse = generate_synthetic_synapse()\n",
    "\n",
    "print(f\"Document: {synapse.document[:100]} ...\")\n",
    "\n",
    "responses: list[chunkSynapse] = validator_dendrite.query(\n",
    "    axons=[top_miner_axon],    \n",
    "    synapse=synapse,\n",
    "    deserialize=False\n",
    ")\n",
    "\n",
    "top_miner_response = responses[0]\n",
    "\n",
    "print(f\"Received {len(top_miner_response.chunks)} chunks from top miner, process time: {top_miner_response.dendrite.process_time}\")\n",
    "print(f\"Chunk 3: {top_miner_response.chunks[2][:50]} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward the top miners response\n",
    "\n",
    "### Reward Mechanism\n",
    "\n",
    "1) Chunks must not have missing words/messed up ordering\n",
    "    - every continguous three word pair from the source document should be in at least 1 chunk\n",
    "    - every word in a single chunk should be in the same order as in the source document\n",
    "2) There is a penalty for chunks that are too long (> `chunk_size` characters)\n",
    "    - Snippet:\n",
    "     ```py\n",
    "     size_penalty += ((chunk_length / chunk_size) - 1) * 10\n",
    "     ```\n",
    "3) Intra-chunk embeddings should have high cosine similarity, inter-chunk embeddings should have low cosine similarity\n",
    "    \n",
    "    - Snippet:\n",
    "    ```py\n",
    "    for i in range(len(testChunks) - 1):\n",
    "        j = i + 1\n",
    "        while j < len(testChunks):\n",
    "            if testChunks[i].sourceChunk == testChunks[j].sourceChunk:\n",
    "                reward += np.dot(np.asarray(embeddings[i]), np.asarray(embeddings[j]))\n",
    "            else:\n",
    "                reward -= np.dot(np.asarray(embeddings[i]), np.asarray(embeddings[j]))\n",
    "            j += 1\n",
    "    ```\n",
    "\n",
    "    - `testChunks` are three sentence chunks formed from each of the returned miner chunks\n",
    "    - `embeddings` are the embeddings of the three sentence chunks, via OpenAI's `text-embedding-ada-002` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top miner reward: 0.0039050038673031375\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from chunking.validator.reward import reward\n",
    "\n",
    "bt.debug()\n",
    "\n",
    "top_miner_reward = reward(self=None, document=synapse.document, chunk_size=synapse.chunk_size, response=top_miner_response, override_client=OpenAI(), override_num_embeddings=50, verbose=True)\n",
    "\n",
    "print(f\"Top miner reward: {top_miner_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group tournament ranking\n",
    "\n",
    "Generally, a miner group is queried by a validator at request time. The default group size is min(metagraph.n, 25). The validator will then rank the miners in the group based on the reward mechanism to make local ranks, then these will be translated into global ranks. Lower rank is better.\n",
    "\n",
    "Here's an example of querying a group of 4 miners (assuming the `group_size` is 4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 4 miners uids: [1 0 2 3]\n",
      "Received 5 chunks from 5FZWySZzkt, process time: 0.11609911918640137\n",
      "Received 5 chunks from 5DMQN4xwHm, process time: 0.1407170295715332\n",
      "Received 5 chunks from 5CDEHMHzvr, process time: 0.14405298233032227\n",
      "Received 5 chunks from 5CzASa8NMS, process time: 0.08191990852355957\n",
      "Rewards: [0.00392112 0.00406185 0.00398069 0.00400092]\n",
      "Response ranks: [3. 0. 2. 1.]\n"
     ]
    }
   ],
   "source": [
    "from chunking.validator.reward import rank_responses\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "top_4_miners_uids = metagraph.I.argsort()[-4:][::-1]\n",
    "\n",
    "print(f\"Top 4 miners uids: {top_4_miners_uids}\")\n",
    "\n",
    "axons = [metagraph.axons[uid] for uid in top_4_miners_uids]\n",
    "\n",
    "responses: list[chunkSynapse] = validator_dendrite.query(\n",
    "    axons=axons,\n",
    "    synapse=synapse,\n",
    "    deserialize=False\n",
    ")\n",
    "\n",
    "for response in responses:\n",
    "    print(f\"Received {len(response.chunks)} chunks from {response.axon.hotkey[:10]}, process time: {response.dendrite.process_time}\")    \n",
    "\n",
    "rewards = np.array([reward(self=None, document=synapse.document, chunk_size=synapse.chunk_size, response=response, override_client=OpenAI(), override_num_embeddings=50, verbose=False) for response in responses])\n",
    "\n",
    "print(f\"Rewards: {rewards}\")\n",
    "\n",
    "response_ranks = rank_responses(rewards)\n",
    "\n",
    "print(f\"Response ranks: {response_ranks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These response ranks would then be translated into global ranks (no change in this case as they are the top 4 miners) and combined with the previous `scores` as a moving average (therefore a lower `score` is better, even if a higher `reward` is better). \n",
    "\n",
    "- Weights are then determined based on the global `scores` (which is basically just ranks as a moving average for all miner UIDs)\n",
    "\n",
    "- So, if the ranks are [2, 0, 3, 1] (0-indexed), the scores might be something like [2.0393, .4593, 2.539, 1.3940] (as it is the moving average of ranks), and the weights would be [0.5, 1, 0.25, 0.75], where each index is a UID."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
