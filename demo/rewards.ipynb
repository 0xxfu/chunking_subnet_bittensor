{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward mechanism demo\n",
    "\n",
    "## Imports + Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chunking.protocol import chunkSynapse\n",
    "import bittensor as bt\n",
    "import requests\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install python-dotenv -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI API key necessary for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if not os.environ.get('OPENAI_API_KEY'):\n",
    "    raise Exception(\"Make sure to set OPENAI_API_KEY in your .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.27187   , 0.31795225, 0.1597467 , 0.14297703, 0.09549096,\n",
       "       0.00793469, 0.00399786], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netuid = 1 # TODO: set netuid\n",
    "network = 'ws://localhost:9946' # TODO: set network\n",
    "\n",
    "metagraph = bt.metagraph(netuid, network)\n",
    "\n",
    "metagraph.I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wallet setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallet_name = \"owner\" # TODO: set wallet name\n",
    "hotkey_name = \"validator-1\" # TODO: set hotkey name\n",
    "\n",
    "validator_wallet = bt.wallet(name=wallet_name, hotkey=hotkey_name)\n",
    "validator_dendrite = bt.dendrite(wallet=validator_wallet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the top miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top miner uid: 1\n",
      "Document: ? (also written Tanda Tanya, meaning Question Mark) is a 2011 Indonesian drama film directed by Hanu ...\n",
      "Received 5 chunks from top miner, process time: 0.04208207130432129\n",
      "Chunk 3: The director of Mahaka Pictures, Erick Thohir, sta ...\n"
     ]
    }
   ],
   "source": [
    "page = 33653136 # fixed article id\n",
    "\n",
    "def generate_synthetic_synapse() -> chunkSynapse:    \n",
    "    document = requests.get('https://en.wikipedia.org/w/api.php', params={\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'pageids': page,\n",
    "        'prop': 'extracts',\n",
    "        'explaintext': True,\n",
    "        'exsectionformat': 'plain',\n",
    "        }).json()['query']['pages'][str(page)]['extract']\n",
    "    document = document.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    document = ' '.join(document.split())\n",
    "    synapse = chunkSynapse(document=document, time_soft_max=5.0, chunk_size=4096)\n",
    "    return synapse\n",
    "\n",
    "\n",
    "top_miner_uid = metagraph.I.argmax().item()\n",
    "\n",
    "print(f\"Top miner uid: {top_miner_uid}\")\n",
    "\n",
    "top_miner_axon = metagraph.axons[top_miner_uid]\n",
    "\n",
    "# Generate the 'synthetic' query: a featured article from wikipedia.\n",
    "synapse = generate_synthetic_synapse()\n",
    "\n",
    "print(f\"Document: {synapse.document[:100]} ...\")\n",
    "\n",
    "responses: list[chunkSynapse] = validator_dendrite.query(\n",
    "    axons=[top_miner_axon],    \n",
    "    synapse=synapse,\n",
    "    deserialize=False\n",
    ")\n",
    "\n",
    "top_miner_response = responses[0]\n",
    "\n",
    "print(f\"Received {len(top_miner_response.chunks)} chunks from top miner, process time: {top_miner_response.dendrite.process_time}\")\n",
    "print(f\"Chunk 3: {top_miner_response.chunks[2][:50]} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward the top miners response\n",
    "\n",
    "### Reward Mechanism\n",
    "\n",
    "1) Chunks must not have missing words/messed up ordering\n",
    "    - every continguous three word pair from the source document should be in at least 1 chunk\n",
    "    - every word in a single chunk should be in the same order as in the source document\n",
    "2) There is a penalty for chunks that are too long (> `chunk_size` characters)\n",
    "    - Snippet:\n",
    "     ```py\n",
    "     size_penalty += ((chunk_length / chunk_size) - 1) * 10\n",
    "     ```\n",
    "3) Intra-chunk embeddings should have high cosine similarity, inter-chunk embeddings should have low cosine similarity\n",
    "    \n",
    "    - Snippet:\n",
    "    ```py\n",
    "    for i in range(len(testChunks) - 1):\n",
    "        j = i + 1\n",
    "        while j < len(testChunks):\n",
    "            if testChunks[i].sourceChunk == testChunks[j].sourceChunk:\n",
    "                reward += np.dot(np.asarray(embeddings[i]), np.asarray(embeddings[j]))\n",
    "            else:\n",
    "                reward -= np.dot(np.asarray(embeddings[i]), np.asarray(embeddings[j]))\n",
    "            j += 1\n",
    "    ```\n",
    "\n",
    "    - `testChunks` are three sentence chunks formed from each of the returned miner chunks\n",
    "    - `embeddings` are the embeddings of the three sentence chunks, via OpenAI's `text-embedding-ada-002` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-21 21:18:51.602 |       INFO       | bittensor:loggingmachine.py:305 | Debug enabled.\n",
      "Chunk 0 has 34 sentences. Added 12 test segments\n",
      "Chunk 1 has 40 sentences. Added 14 test segments\n",
      "Chunk 2 has 34 sentences. Added 12 test segments\n",
      "Chunk 3 has 34 sentences. Added 12 test segments\n",
      "Chunk 4 has 6 sentences. Added 2 test segments\n",
      "Every set of 3 adjacent words from the document appears in the chunks\n",
      "Using 50 test segments for evaluation\n",
      "Calculated embeddings for 50 test segments\n",
      "Embedding reward: -538.3118926486421\n",
      "Size penalty: 0\n",
      "Ensuring reward is positive (1.01 ** reward):\n",
      "0.004717947460397132\n",
      "Top miner reward: 0.004717947460397132\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from chunking.validator.reward import reward\n",
    "\n",
    "bt.debug()\n",
    "\n",
    "top_miner_reward = reward(self=None, document=synapse.document, chunk_size=synapse.chunk_size, response=top_miner_response, override_client=OpenAI(), override_num_embeddings=50, verbose=True)\n",
    "\n",
    "print(f\"Top miner reward: {top_miner_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group tournament ranking\n",
    "\n",
    "Generally, a miner group is queried by a validator at request time. The default group size is `min(metagraph.n, 25)`. The validator will then rank the miners in this group relative to each other based on the incentive mechanism. These local ranks will be then translated into global ranks. Having a lower overall rank is better.\n",
    "\n",
    "Here's an example of querying a group of 4 miners (assuming the `group_size` is 4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 4 miners uids: [1 0 2 3]\n",
      "Received 5 chunks from 5FZWySZzkt, process time: 0.16794490814208984\n",
      "Received 5 chunks from 5DMQN4xwHm, process time: 0.17258810997009277\n",
      "Received 5 chunks from 5CDEHMHzvr, process time: 0.07403016090393066\n",
      "Received 5 chunks from 5CzASa8NMS, process time: 0.07992315292358398\n",
      "Rewards: [0.00396825 0.00403438 0.00393537 0.00385683]\n",
      "Response ranks: [1. 0. 2. 3.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-21 21:18:51.913 |      DEBUG       | bittensor:loggingmachine.py:354 |  -  - \n",
      "2024-07-21 21:18:51.934 |      DEBUG       | bittensor:loggingmachine.py:354 |  -  - \n",
      "2024-07-21 21:18:51.952 |      DEBUG       | bittensor:loggingmachine.py:354 |  -  - \n",
      "2024-07-21 21:18:51.972 |      DEBUG       | bittensor:loggingmachine.py:354 |  -  - \n",
      "2024-07-21 21:18:52.026 |       INFO       | bittensor:loggingmachine.py:359 |  - Ignoring error when setting attribute: 1 validation error for chunkSynapse\n",
      "computed_body_hash\n",
      "  Field is frozen [type=frozen_field, input_value='', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.8/v/frozen_field - \n",
      "2024-07-21 21:18:52.052 |       INFO       | bittensor:loggingmachine.py:359 |  - Ignoring error when setting attribute: 1 validation error for chunkSynapse\n",
      "computed_body_hash\n",
      "  Field is frozen [type=frozen_field, input_value='', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.8/v/frozen_field - \n",
      "2024-07-21 21:18:52.081 |       INFO       | bittensor:loggingmachine.py:359 |  - Ignoring error when setting attribute: 1 validation error for chunkSynapse\n",
      "computed_body_hash\n",
      "  Field is frozen [type=frozen_field, input_value='', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.8/v/frozen_field - \n",
      "2024-07-21 21:18:52.106 |       INFO       | bittensor:loggingmachine.py:359 |  - Ignoring error when setting attribute: 1 validation error for chunkSynapse\n",
      "computed_body_hash\n",
      "  Field is frozen [type=frozen_field, input_value='', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.8/v/frozen_field - \n"
     ]
    }
   ],
   "source": [
    "from chunking.validator.reward import rank_responses\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "top_4_miners_uids = metagraph.I.argsort()[-4:][::-1]\n",
    "\n",
    "print(f\"Top 4 miners uids: {top_4_miners_uids}\")\n",
    "\n",
    "axons = [metagraph.axons[uid] for uid in top_4_miners_uids]\n",
    "\n",
    "responses: list[chunkSynapse] = validator_dendrite.query(\n",
    "    axons=axons,\n",
    "    synapse=synapse,\n",
    "    deserialize=False\n",
    ")\n",
    "\n",
    "for response in responses:\n",
    "    print(f\"Received {len(response.chunks)} chunks from {response.axon.hotkey[:10]}, process time: {response.dendrite.process_time}\")    \n",
    "\n",
    "rewards = np.array([reward(self=None, document=synapse.document, chunk_size=synapse.chunk_size, response=response, override_client=OpenAI(), override_num_embeddings=50, verbose=False) for response in responses])\n",
    "\n",
    "print(f\"Rewards: {rewards}\")\n",
    "\n",
    "response_ranks = rank_responses(rewards)\n",
    "\n",
    "print(f\"Response ranks: {response_ranks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These response ranks would then be translated into global ranks (no change in this case as they are the top 4 miners) and combined with the previous `scores` as a moving average (therefore a lower `score` is better, even if a higher `reward` is better). \n",
    "\n",
    "- Weights are then determined based on the global `scores` (which is basically just ranks as a moving average for all miner UIDs)\n",
    "\n",
    "- So, if the ranks are [2, 0, 3, 1] (0-indexed), the scores might be something like [2.0393, .4593, 2.539, 1.3940] (as it is the moving average of ranks), and the weights would be [0.5, 1, 0.25, 0.75], where each index is a UID."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
