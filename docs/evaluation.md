# Evaluation

Described in more detail in the validator and mining setup documentation, validators need to consider the number of embeddings they will generate while evaluating a miner. When scoring, a random-sample of 3-sentence segments is taken from the response and is then embedded. The dot product of every possible pair of these embeddings is then compared. If the embeddings originated from the same chunk, it is added to the final score, whereas if the embeddings originated from different chunks, it is subtracted from the final score.

Here is a visualization of how the validator calculates a minerâ€™s score:

![evaluations](../assets/evaluations.png)

Taking a greater sample size will likely result in more accurate evaluations and higher yields, although this will come at the cost of more API calls to generate the additional embeddings and potentially more time and resources comparing them against each other.

# Penalties

If the chunks generated by the miner have more tokens than specified by the validator, their score is penalized exponentially for each token above the limit.

Finally, note that there is a soft-time limit, currently set to 5 seconds. Validators exponentially penalize responses for each second they are late.
```python
reward *= (2/3) ** over_time
```